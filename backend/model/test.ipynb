{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append(\"..\")\n",
    "from utils.preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ExportModel(tf.Module):\n",
    "#   def __init__(self, model):\n",
    "#     self.model = model\n",
    "\n",
    "#     # Accept either a string-filename or a batch of waveforms.\n",
    "#     # YOu could add additional signatures for a single wave, or a ragged-batch. \n",
    "#     self.__call__.get_concrete_function(\n",
    "#         x=tf.TensorSpec(shape=(), dtype=tf.string))\n",
    "#     self.__call__.get_concrete_function(\n",
    "#        x=tf.TensorSpec(shape=[None, 16000], dtype=tf.float32))\n",
    "\n",
    "\n",
    "#   @tf.function\n",
    "#   def __call__(self, x):\n",
    "#     # If they pass a string, load the file and decode it. \n",
    "#     if x.dtype == tf.string:\n",
    "#       x = tf.io.read_file(x)\n",
    "#       x, _ = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000,)\n",
    "#       x = tf.squeeze(x, axis=-1)\n",
    "#       x = x[tf.newaxis, :]\n",
    "\n",
    "#     x = get_spectrogram(x)  \n",
    "#     result = self.model(x, training=False)\n",
    "\n",
    "#     class_ids = tf.argmax(result, axis=-1)\n",
    "#     class_names = tf.gather(label_names, class_ids)\n",
    "#     return {'predictions':result,\n",
    "#             'class_ids': class_ids,\n",
    "#             'class_names': class_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_spectrogram(waveform):\n",
    "#   # Convert the waveform to a spectrogram via a STFT.\n",
    "#   spectrogram = tf.signal.stft(\n",
    "#       waveform, frame_length=255, frame_step=128)\n",
    "#   # Obtain the magnitude of the STFT.\n",
    "#   spectrogram = tf.abs(spectrogram)\n",
    "#   # Add a `channels` dimension, so that the spectrogram can be used\n",
    "#   # as image-like input data with convolution layers (which expect\n",
    "#   # shape (`batch_size`, `height`, `width`, `channels`).\n",
    "#   spectrogram = spectrogram[..., tf.newaxis]\n",
    "#   return spectrogram\n",
    "\n",
    "# def preprocess(x):\n",
    "#     x = tf.io.read_file(x)\n",
    "#     x, _ = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000,)\n",
    "#     x = tf.squeeze(x, axis=-1)\n",
    "#     x = x[tf.newaxis, :]\n",
    "\n",
    "#     x = get_spectrogram(x)  \n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export = ExportModel(model)\n",
    "# export(tf.constant(str(data_dir/'no/01bb6a2a_nohash_0.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_PATH = 'data/mini_speech_commands'\n",
    "\n",
    "# data_dir = pathlib.Path(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      accel_x   accel_y   accel_z\n",
      "0    0.070068 -1.013977  0.104187\n",
      "1   -0.044434 -0.960999  0.007507\n",
      "2    0.000244 -0.932434 -0.016663\n",
      "3   -0.043457 -0.956360  0.100525\n",
      "4    0.015381 -1.002502  0.049988\n",
      "..        ...       ...       ...\n",
      "695 -0.003662 -0.681946 -0.146545\n",
      "696 -0.065430 -0.805481 -0.159485\n",
      "697 -0.011719 -0.970520 -0.095520\n",
      "698 -0.107178 -1.269348 -0.158752\n",
      "699 -0.046387 -1.302307 -0.083557\n",
      "\n",
      "[700 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "input_directory = \"../assets/pdiot_submission/s2029523\"\n",
    "\n",
    "data_list = []\n",
    "label_list = []\n",
    "total_rows = 0\n",
    "\n",
    "for file in os.listdir(input_directory):\n",
    "    if file.endswith(\".csv\") and \"unprocessed\" not in file:\n",
    "        filename = os.path.join(input_directory, file)\n",
    "        label, _, _ = extract_activity_and_status(filename)\n",
    "\n",
    "        df = pd.read_csv(filename, usecols=[1,2,3])\n",
    "\n",
    "        # Determine the number of rows to take from this file\n",
    "        rows_to_take = min(700 - total_rows, len(df))\n",
    "        \n",
    "        # Update the df to only contain the necessary rows and update our counter\n",
    "        df = df.head(rows_to_take)\n",
    "        label_list.append(label)\n",
    "        data_list.append(df)\n",
    "\n",
    "# all_data = pd.concat(data_list, ignore_index=True)\n",
    "print(data_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Normalizing sensor readings\n",
    "scaler = StandardScaler()\n",
    "\n",
    "sensor_columns = ['accel_x', 'accel_y', 'accel_z']\n",
    "\n",
    "scaler.fit(data_list[0][sensor_columns])\n",
    "\n",
    "for i in range(len(data_list)):\n",
    "    data_list[i][sensor_columns] = scaler.transform(data_list[i][sensor_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert data and labels to numpy arrays\n",
    "X = np.array([df.values for df in data_list])\n",
    "X = np.stack(X)\n",
    "y = LabelEncoder().fit_transform(label_list)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Convert X_train to float32\n",
    "X_train = X_train.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.2492833e-04 1.3478450e-03 7.0554376e-01 1.0452529e-02 3.5982503e-04\n",
      "  4.4327203e-02 4.6993850e-04 6.6712382e-03 7.1605816e-02 6.5388018e-03\n",
      "  1.4824677e-01 3.8113627e-03]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"../exported_models/model_combined.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load and preprocess the audio file\n",
    "# file_path = str(data_dir/'no/01bb6a2a_nohash_0.wav')\n",
    "# data = preprocess(file_path)\n",
    "\n",
    "# Ensure the input data is the correct shape and type\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(X_train[0], dtype=input_details[0]['dtype']).reshape(input_shape)\n",
    "\n",
    "# Set the tensor and invoke the interpreter\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output tensor\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdiot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

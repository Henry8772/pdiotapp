{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, regularizers\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.preprocess import *\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_path = '../exported_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = \"../assets/Respeck\"\n",
    "\n",
    "data_list = []\n",
    "label_list = []\n",
    "total_rows = 0\n",
    "\n",
    "all_occu = {}\n",
    "\n",
    "\n",
    "\n",
    "# Define window size and overlap\n",
    "window_size = 50\n",
    "overlap_size = 25  # This is often set to 50% of the window size\n",
    "\n",
    "all_occu = {}\n",
    "data_windows = []\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for folder_name in os.listdir(input_directory):\n",
    "    folder_path = os.path.join(input_directory, folder_name)\n",
    "    if folder_name == \".DS_Store\":\n",
    "        continue\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".csv\") and \"unprocessed\" not in file and 'normal' in file:\n",
    "            \n",
    "            filename = os.path.join(folder_path, file)\n",
    "            activity, activity_sub, _ = extract_activity_and_status(filename)\n",
    "            if \"sitting\" in activity or \"standing\" in activity:\n",
    "                label = 'sitting/standing' + \"_\" + activity_sub\n",
    "            else:\n",
    "                label = activity + \"_\" + activity_sub\n",
    "            \n",
    "            \n",
    "            if label not in all_occu:\n",
    "                all_occu[label] = {'count': 1, 'directories': [filename]}\n",
    "            else:\n",
    "                all_occu[label]['count'] += 1\n",
    "                all_occu[label]['directories'].append(filename)\n",
    "            \n",
    "            df = pd.read_csv(filename, usecols=[2, 3, 4])\n",
    "\n",
    "            # Determine the number of rows to take from this file\n",
    "            rows_to_take = min(700 - total_rows, len(df))\n",
    "            \n",
    "            # Update the df to only contain the necessary rows and update our counter\n",
    "            df = df.head(rows_to_take)\n",
    "\n",
    "            # Apply sliding window technique\n",
    "            for start in range(0, len(df), window_size - overlap_size):\n",
    "                end = start + window_size\n",
    "                window = df.iloc[start:end].values  # Convert the window to a NumPy array\n",
    "                if len(window) < 50:\n",
    "                    continue\n",
    "\n",
    "                # Normalize the window data\n",
    "                # if not data_windows:  # If this is the first window, fit the scaler\n",
    "                #     scaler.fit(window)\n",
    "                # window_normalized = scaler.transform(window)  # Transform the data\n",
    "                \n",
    "                # # Add the normalized window and label to the lists\n",
    "                data_windows.append(window)\n",
    "                label_list.append(label)\n",
    "# print(data_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "14904\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(data_windows[0]))\n",
    "print(len(data_windows))\n",
    "\n",
    "wrong_list = []\n",
    "\n",
    "for data in data_windows:\n",
    "    if len(data) != 50:\n",
    "        wrong_list.append(data)\n",
    "\n",
    "print(len(wrong_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_label_list = list(set(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['normal walking_normal',\n",
       " 'lying down on left_normal',\n",
       " 'lying down on stomach_normal',\n",
       " 'miscellaneous movements_normal',\n",
       " 'sitting/standing_normal',\n",
       " 'ascending stairs_normal',\n",
       " 'lying down right_normal',\n",
       " 'lying down back_normal',\n",
       " 'running_normal',\n",
       " 'shuffle walking_normal',\n",
       " 'descending stairs_normal']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_offline(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        # CNN layers for spatial feature extraction\n",
    "        layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "        layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "        layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        # Flatten the output of the CNN to feed into the LSTM\n",
    "        layers.Flatten(),\n",
    "\n",
    "        # Add LSTM layers, making sure the first one returns sequences\n",
    "        layers.Reshape((-1, 256)), # Reshape the flattened output to be sequence-like if necessary\n",
    "        layers.LSTM(256, return_sequences=True, kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.LSTM(256, kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "\n",
    "        # Dense layers for final classification\n",
    "        layers.Dense(64, activation='relu'),  # First fully connected layer\n",
    "        layers.Dense(32, activation='relu'),  # Second fully connected layer\n",
    "        layers.Dense(num_classes, activation='softmax'),  # Output layer\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([df for df in data_windows])\n",
    "X = np.stack(X)\n",
    "y = LabelEncoder().fit_transform(label_list)\n",
    "\n",
    "# Convert X to float32 and y to categorical (for softmax)\n",
    "X = X.astype('float32')\n",
    "y_categorical = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in label_list:\n",
    "    if n not in overall_dict:\n",
    "        overall_dict[n] = 1\n",
    "    elif n in overall_dict:\n",
    "        overall_dict[n] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shuffle walking_normal': 1242,\n",
       " 'lying down on stomach_normal': 1242,\n",
       " 'ascending stairs_normal': 1242,\n",
       " 'sitting/standing_normal': 2484,\n",
       " 'running_normal': 1242,\n",
       " 'lying down right_normal': 1242,\n",
       " 'descending stairs_normal': 1242,\n",
       " 'miscellaneous movements_normal': 1242,\n",
       " 'normal walking_normal': 1242,\n",
       " 'lying down on left_normal': 1242,\n",
       " 'lying down back_normal': 1242}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the Leave-One-Out cross-validator\n",
    "# loo = LeaveOneOut()\n",
    "\n",
    "# # Lists to store scores for each fold\n",
    "# accuracy_scores = []\n",
    "\n",
    "# # Iterate over each train-test split\n",
    "# for train_index, test_index in loo.split(X):\n",
    "#     # Split the data into training and test set for the current fold\n",
    "#     X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
    "#     y_train_fold, y_test_fold = y_categorical[train_index], y_categorical[test_index]\n",
    "\n",
    "#     # Define input_shape and num_classes based on the training data\n",
    "#     input_shape = X_train_fold.shape[1:]  # (timesteps, features)\n",
    "#     num_classes = y_train_fold.shape[1]   # number of categories\n",
    "\n",
    "#     # Create a new model instance (define create_model function as per your model architecture)\n",
    "#     model = create_model_offline(input_shape, num_classes)\n",
    "\n",
    "#     # Train the model\n",
    "#     model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "#     # Evaluate the model on the test set\n",
    "#     loss, accuracy = model.evaluate(X_test_fold, y_test_fold, verbose=0)\n",
    "#     accuracy_scores.append(accuracy)\n",
    "\n",
    "# # Calculate the average accuracy over all LOO folds\n",
    "# average_accuracy = np.mean(accuracy_scores)\n",
    "# print(f'LOO Cross-Validation Accuracy: {average_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model on the test data\n",
    "# scores = model.evaluate(X_test_fold, y_test_fold, verbose=0)\n",
    "# accuracy_scores.append(scores[1])  # Assuming that scores[1] is accuracy if you followed the standard compile\n",
    "\n",
    "# # The final accuracy is the mean of all fold accuracies\n",
    "# average_accuracy = np.mean(accuracy_scores)\n",
    "# print(f'LOO Cross-Validation Accuracy: {average_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_online(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        # CNN layers for spatial feature extraction\n",
    "        layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "        layers.Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "        layers.Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "\n",
    "        #  # GRU layers\n",
    "        # layers.GRU(128, return_sequences=True, \n",
    "        #            kernel_regularizer=regularizers.l2(0.001)),  # L2 Regularization\n",
    "        # layers.GRU(64, kernel_regularizer=regularizers.l2(0.001)),  # L2 Regularization\n",
    "        # layers.BatchNormalization(),\n",
    "        # layers.Dropout(0.5),  # Dropout\n",
    "\n",
    "        # Dense layers for final classification\n",
    "        layers.Dense(64, activation='relu'),  # First fully connected layer\n",
    "        layers.Dense(32, activation='relu'),  # Second fully connected layer\n",
    "        layers.Dense(num_classes, activation='softmax'),  # Output layer\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14904,)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(label_list).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.20678711 -0.958313   -0.11602783]\n",
      " [-0.32202148 -0.89263916 -0.00396729]\n",
      " [-0.16381836 -1.0549927  -0.07769775]\n",
      " [-0.33276367 -0.88067627 -0.03253174]\n",
      " [-0.23950195 -0.7312622  -0.02789307]\n",
      " [-0.12988281 -0.68292236 -0.02496338]\n",
      " [-0.19824219 -0.53912354  0.05218506]\n",
      " [-0.26782227 -0.6939087   0.03582764]\n",
      " [-0.2084961  -0.88775635  0.01580811]\n",
      " [-0.20727539 -0.9451294   0.01776123]\n",
      " [-0.2824707  -1.0991821   0.15301514]\n",
      " [-0.37817383 -1.2046509   0.16131592]\n",
      " [-0.45996094 -1.2398071  -0.07647705]\n",
      " [-0.27734375 -1.3796997  -0.46221924]\n",
      " [-0.23120117 -1.4768677  -0.2600708 ]\n",
      " [-0.4345703  -0.89312744 -0.17633057]\n",
      " [-0.39770508 -0.5862427  -0.40264893]\n",
      " [-0.00219727 -0.9783325  -0.25299072]\n",
      " [-0.53027344 -1.171936   -0.13311768]\n",
      " [-0.41601562 -0.66851807 -0.17926025]\n",
      " [-0.1940918  -0.66607666 -0.09796143]\n",
      " [-0.15795898 -0.6555786  -0.09307861]\n",
      " [-0.32202148 -0.5964966  -0.11529541]\n",
      " [-0.22290039 -0.6929321  -0.13116455]\n",
      " [-0.3684082  -0.81744385 -0.10870361]\n",
      " [-0.50097656 -0.88775635 -0.15264893]\n",
      " [-0.6772461  -1.2061157  -0.19830322]\n",
      " [-0.58813477 -1.3235474  -0.2288208 ]\n",
      " [-0.4116211  -1.2178345  -0.48590088]\n",
      " [-0.07714844 -1.2053833  -0.39019775]\n",
      " [-0.28076172 -0.6956177  -0.35528564]\n",
      " [-0.31933594 -0.6983032  -0.10113525]\n",
      " [-0.29785156 -0.6951294  -0.02813721]\n",
      " [-0.23095703 -0.677063   -0.03887939]\n",
      " [-0.2692871  -0.6553345  -0.09405518]\n",
      " [-0.18383789 -0.63604736 -0.02008057]\n",
      " [-0.41992188 -0.77056885  0.07293701]\n",
      " [-0.2939453  -0.8428345  -0.03131104]\n",
      " [-0.3173828  -1.1482544   0.10394287]\n",
      " [-0.6291504  -1.3919067  -0.12115479]\n",
      " [-0.07006836 -1.21344    -0.21734619]\n",
      " [-0.3076172  -0.9522095  -0.26031494]\n",
      " [-0.30908203 -0.986145   -0.2000122 ]\n",
      " [-0.40014648 -0.79034424 -0.31524658]\n",
      " [-0.3244629  -0.8257446  -0.17901611]\n",
      " [-0.37597656 -0.7810669  -0.15679932]\n",
      " [-0.421875   -0.64630127 -0.19659424]\n",
      " [-0.29785156 -0.7542114  -0.22369385]\n",
      " [-0.2368164  -0.8345337  -0.17877197]\n",
      " [-0.41137695 -0.77227783 -0.11602783]]\n",
      "[1 1 1 ... 1 1 1] [0 2 2 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming `x` is your dataset with the shape (14904, 50, 3)\n",
    "# And `label_list` is a list or numpy array with the length 14904\n",
    "\n",
    "# Convert label_list to a numpy array if it isn't already\n",
    "label_list = np.array(label_list)\n",
    "\n",
    "# Initialize dictionaries to hold the min and max flags for each label\n",
    "min_flags_by_label = {}\n",
    "max_flags_by_label = {}\n",
    "\n",
    "# Loop over each unique label\n",
    "for label in np.unique(label_list)[:1]:\n",
    "    # Find the indices where the label occurs\n",
    "    indices = np.where(label_list == label)[0]\n",
    "    \n",
    "    # Select the data points corresponding to the current label\n",
    "    data_points = X[indices]\n",
    "    print(data_points[0])\n",
    "    \n",
    "    # Reshape data points to a 2D array where each row is a time point and columns are x, y, z\n",
    "    reshaped_points = data_points.reshape(-1, 3)\n",
    "    \n",
    "\n",
    "    # Find the indices of the min and max values across the x, y, z axis\n",
    "    min_indices = np.argmin(reshaped_points, axis=1)\n",
    "    max_indices = np.argmax(reshaped_points, axis=1)\n",
    "    print(min_indices, max_indices)\n",
    "\n",
    "    # Convert these indices to a one-hot encoded format\n",
    "    min_one_hot = np.eye(3)[min_indices]\n",
    "    max_one_hot = np.eye(3)[max_indices]\n",
    "\n",
    "    # Aggregate the one-hot encoded min and max indicators for the current label\n",
    "    min_aggregated = np.sum(min_one_hot, axis=0)\n",
    "    max_aggregated = np.sum(max_one_hot, axis=0)\n",
    "\n",
    "    # Store the aggregated min and max flags in the dictionaries, keyed by label\n",
    "    min_flags_by_label[label] = min_aggregated\n",
    "    max_flags_by_label[label] = max_aggregated\n",
    "\n",
    "# Now min_flags_by_label and max_flags_by_label dictionaries hold the min and max flags for each label.\n",
    "# The flags indicate the count of how many times each axis (x, y, z) was the minimum or maximum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ascending stairs_normal': array([37134.,  1979., 22987.])}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_flags_by_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ascending stairs_normal': array([ 2100., 58984.,  1016.])}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_flags_by_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input_shape and num_classes based on the full dataset\n",
    "input_shape = X.shape[1:]  # (timesteps, features)\n",
    "num_classes = y_categorical.shape[1]   # number of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [[0, 0, -1]] * 50\n",
    "test_input_data = np.array(test_list).reshape(input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373/373 [==============================] - 3s 8ms/step - loss: 0.7887 - accuracy: 0.7311 - val_loss: 1.1748 - val_accuracy: 0.7139\n",
      "Epoch 2/10\n",
      "373/373 [==============================] - 3s 7ms/step - loss: 0.4135 - accuracy: 0.8633 - val_loss: 0.3899 - val_accuracy: 0.8739\n",
      "Epoch 3/10\n",
      "373/373 [==============================] - 3s 9ms/step - loss: 0.3076 - accuracy: 0.9000 - val_loss: 0.3048 - val_accuracy: 0.8910\n",
      "Epoch 4/10\n",
      "373/373 [==============================] - 3s 9ms/step - loss: 0.2388 - accuracy: 0.9241 - val_loss: 0.3788 - val_accuracy: 0.8916\n",
      "Epoch 5/10\n",
      "373/373 [==============================] - 3s 8ms/step - loss: 0.1920 - accuracy: 0.9366 - val_loss: 0.3094 - val_accuracy: 0.9057\n",
      "Epoch 6/10\n",
      "373/373 [==============================] - 3s 8ms/step - loss: 0.1806 - accuracy: 0.9412 - val_loss: 0.2672 - val_accuracy: 0.9148\n",
      "Epoch 7/10\n",
      "373/373 [==============================] - 3s 8ms/step - loss: 0.1611 - accuracy: 0.9468 - val_loss: 0.2957 - val_accuracy: 0.9108\n",
      "Epoch 8/10\n",
      "373/373 [==============================] - 3s 9ms/step - loss: 0.1477 - accuracy: 0.9514 - val_loss: 0.3249 - val_accuracy: 0.8990\n",
      "Epoch 9/10\n",
      "373/373 [==============================] - 3s 9ms/step - loss: 0.1303 - accuracy: 0.9588 - val_loss: 0.3286 - val_accuracy: 0.9148\n",
      "Epoch 10/10\n",
      "373/373 [==============================] - 3s 8ms/step - loss: 0.1038 - accuracy: 0.9657 - val_loss: 0.3210 - val_accuracy: 0.9198\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3210 - accuracy: 0.9198\n",
      "Validation Accuracy: 0.9198\n"
     ]
    }
   ],
   "source": [
    "# Create a new model instance using the defined architecture\n",
    "model_online = create_model_online(input_shape, num_classes)\n",
    "\n",
    "# Split the dataset into training and validation sets if needed\n",
    "# If you want to use the full dataset for training, you can skip this step\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "history = model_online.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    validation_data=(X_val, y_val)  # Omit this if not using a validation set\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set if it was used\n",
    "if 'X_val' in locals():\n",
    "    val_loss, val_accuracy = model_online.evaluate(X_val, y_val, verbose=1)\n",
    "    print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "\n",
    "\n",
    "# If you have a separate test set, evaluate on it\n",
    "# test_loss, test_accuracy = model.evaluate([test_input_data], [], verbose=1)\n",
    "# print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n"
     ]
    }
   ],
   "source": [
    "result = model_online.predict(np.array([test_input_data]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.6428309e-15, 1.5954474e-15, 9.8547364e-13, 4.3122473e-05,\n",
       "        9.9995685e-01, 2.1046068e-10, 1.4687242e-08, 1.4004842e-14,\n",
       "        2.4372910e-11, 1.7870778e-20, 9.4859840e-12]], dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sitting/standing_normal'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_label_list[np.argmax(result)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/gg/10wpd3jj5v7dthydfjzl0gcm0000gn/T/tmpveuzjy4t/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/gg/10wpd3jj5v7dthydfjzl0gcm0000gn/T/tmpveuzjy4t/assets\n",
      "2023-11-08 12:40:18.080536: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-11-08 12:40:18.080548: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-11-08 12:40:18.080658: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /var/folders/gg/10wpd3jj5v7dthydfjzl0gcm0000gn/T/tmpveuzjy4t\n",
      "2023-11-08 12:40:18.081936: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2023-11-08 12:40:18.081941: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /var/folders/gg/10wpd3jj5v7dthydfjzl0gcm0000gn/T/tmpveuzjy4t\n",
      "2023-11-08 12:40:18.086520: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-11-08 12:40:18.147559: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /var/folders/gg/10wpd3jj5v7dthydfjzl0gcm0000gn/T/tmpveuzjy4t\n",
      "2023-11-08 12:40:18.165123: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 84466 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# tflite_model_test = converter.convert()\n",
    "\n",
    "# with open(\"model.tflite\", \"wb\") as f:\n",
    "#     f.write(tflite_model_test)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_online)\n",
    "\n",
    "# Allow for TensorFlow ops that aren't natively supported in TFLite\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS\n",
    "]\n",
    "\n",
    "# # Disable the lowering of tensor list operations\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "\n",
    "tflite_model_test = converter.convert()\n",
    "\n",
    "with open(exported_path + \"model_cnn.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdiot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
